{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7ff7d0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfc54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a7455",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d103e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier Pickle\n",
    "train_data_path = Path('data/train_data.pkl')\n",
    "with open(train_data_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "images = data['images']\n",
    "labels = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9e6aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1080, 28, 28, 3), (1080, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e3efe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28, 3), (1,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ex = images[0]\n",
    "label_ex = labels[0]\n",
    "image_ex.shape, label_ex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e653a",
   "metadata": {},
   "source": [
    "Notre CNN nécessitera le nombre de canaux de nos images (grayscale ou RGB) et la taille de notre image :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe02940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_of_channels = image_ex.shape[2]\n",
    "dimension_of_image = image_ex.shape[0]\n",
    "nb_of_channels, dimension_of_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf35b5",
   "metadata": {},
   "source": [
    "Pour connaître le nombre de classes que l'on peut prédire, on fait :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6317b09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_of_classes = len(np.unique(labels.flatten()))\n",
    "nb_of_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a564a",
   "metadata": {},
   "source": [
    "# Architecture Design & HPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fbb31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int, dimension_of_image: int, conv_kernel_size: int = 3):\n",
    "        \"\"\"\n",
    "        L'architecture de ce CNN est pour le moment arbitraire, mais nous commençons avec une\n",
    "        convolution (CONV), puis par un max-pooling (POOL) suivi d'une 2ième convolution avec \n",
    "        son 2ième max-pooling (POOL) et finalement notre couche complètement connectée (FC).\n",
    "        \n",
    "        :param in_channels: Le nombre de canaux dans notre image.\n",
    "        :type in_channels: int\n",
    "        :param num_classes: Le nombre de classes que l'on veut prédire. \n",
    "        :type num_classes: int\n",
    "        :param dimension_of_image: La taille de l'image (ex: 28x28),\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        nb_pools = 2\n",
    "        pool_kernel_size = 2\n",
    "        last_conv_out_channels = 16\n",
    "\n",
    "        dim_after_convs = dimension_of_image\n",
    "        for _ in range(nb_pools):\n",
    "            dim_after_convs = dim_after_convs // pool_kernel_size\n",
    "\n",
    "        fc_dimension = last_conv_out_channels * dim_after_convs * dim_after_convs\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=8, \n",
    "            kernel_size=conv_kernel_size, \n",
    "            stride=1, \n",
    "            padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=8, \n",
    "            out_channels=last_conv_out_channels, \n",
    "            kernel_size=conv_kernel_size, \n",
    "            stride=1, \n",
    "            padding=1\n",
    "        )\n",
    "        self.fc = nn.Linear(fc_dimension, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x= self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57504de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device = 'cuda'\n",
    "elif torch.mps.is_available(): device = 'mps' \n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafbec4",
   "metadata": {},
   "source": [
    "Nos hyper-paramètres autres que la taille de l'entrée (`dimension_of_image`), le nombre de canaux (`nb_of_channels`) et le nombre de classes (`nb_of_classes`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9d2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8f142",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120b572",
   "metadata": {},
   "source": [
    "On sépare nos données d'entraînement de nos données de validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd91b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.2\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(images, labels.flatten(), test_size=valid_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067ffc7",
   "metadata": {},
   "source": [
    "On doit normaliser les données, les convertir en tenseurs et les permutter pour que les canaux soit en 2e position :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1df0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train / 255, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "X_valid = torch.tensor(X_valid / 255, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97dd55",
   "metadata": {},
   "source": [
    "On groupe le tout sous forme de \"dataset\" pour nos \"dataloaders\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7547526",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "\n",
    "train_dataset.train = True\n",
    "valid_dataset.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6dff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25b3c9",
   "metadata": {},
   "source": [
    "# Model Creation & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12249c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(in_channels=nb_of_channels, num_classes=nb_of_classes, dimension_of_image=dimension_of_image).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6d910d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e2b2414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===== Epoch [10/100] done ===== \n",
      " ===== Epoch [20/100] done ===== \n",
      " ===== Epoch [30/100] done ===== \n",
      " ===== Epoch [40/100] done ===== \n",
      " ===== Epoch [50/100] done ===== \n",
      " ===== Epoch [60/100] done ===== \n",
      " ===== Epoch [70/100] done ===== \n",
      " ===== Epoch [80/100] done ===== \n",
      " ===== Epoch [90/100] done ===== \n",
      " ===== Epoch [100/100] done ===== \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Mise à jour des paramètres\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f' ===== Epoch [{epoch}/{num_epochs}] done ===== ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fba899",
   "metadata": {},
   "source": [
    "# Checking Accuracy (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0b7c56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on training data')\n",
    "    else:\n",
    "        print('Checking accuracy on validation data')\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}%')\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "dc38dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 695/864 with accuracy 80.44%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3f5f07aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on validation data\n",
      "Got 93/216 with accuracy 43.06%\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(val_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kaggle_2)",
   "language": "python",
   "name": "kaggle_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
